{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "zu0Iuy3KO0cA",
      "metadata": {
        "id": "zu0Iuy3KO0cA"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "BaJLWi0dO3eC",
      "metadata": {
        "id": "BaJLWi0dO3eC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5017d6ee-a8bc-467e-bd53-5e49fc330870"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_google_vertexai[all,anthropic]\n",
            "  Downloading langchain_google_vertexai-2.0.27-py3-none-any.whl.metadata (4.8 kB)\n",
            "\u001b[33mWARNING: langchain-google-vertexai 2.0.27 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting anthropic<1,>=0.35.0 (from anthropic[vertexai]<1,>=0.35.0; extra == \"anthropic\"->langchain_google_vertexai[all,anthropic])\n",
            "  Downloading anthropic-0.58.2-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: bottleneck<2.0.0,>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_vertexai[all,anthropic]) (1.4.2)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.97.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_vertexai[all,anthropic]) (1.104.0)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_vertexai[all,anthropic]) (2.19.0)\n",
            "Requirement already satisfied: httpx<0.29.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_vertexai[all,anthropic]) (0.28.1)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_google_vertexai[all,anthropic])\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3.67 in /usr/local/lib/python3.11/dist-packages (from langchain_google_vertexai[all,anthropic]) (0.3.69)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.6 in /usr/local/lib/python3.11/dist-packages (from langchain_google_vertexai[all,anthropic]) (2.11.0)\n",
            "Collecting pyarrow<20.0.0,>=19.0.1 (from langchain_google_vertexai[all,anthropic])\n",
            "  Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.9 in /usr/local/lib/python3.11/dist-packages (from langchain_google_vertexai[all,anthropic]) (2.11.7)\n",
            "Collecting validators<1,>=0.22.0 (from langchain_google_vertexai[all,anthropic])\n",
            "  Downloading validators-0.35.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.35.0->anthropic[vertexai]<1,>=0.35.0; extra == \"anthropic\"->langchain_google_vertexai[all,anthropic]) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.35.0->anthropic[vertexai]<1,>=0.35.0; extra == \"anthropic\"->langchain_google_vertexai[all,anthropic]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.35.0->anthropic[vertexai]<1,>=0.35.0; extra == \"anthropic\"->langchain_google_vertexai[all,anthropic]) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.35.0->anthropic[vertexai]<1,>=0.35.0; extra == \"anthropic\"->langchain_google_vertexai[all,anthropic]) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.35.0->anthropic[vertexai]<1,>=0.35.0; extra == \"anthropic\"->langchain_google_vertexai[all,anthropic]) (4.14.1)\n",
            "\u001b[33mWARNING: anthropic 0.58.2 does not provide the extra 'vertexai'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bottleneck<2.0.0,>=1.4.2->langchain_google_vertexai[all,anthropic]) (2.0.2)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.97.0->langchain_google_vertexai[all,anthropic]) (2.25.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.97.0->langchain_google_vertexai[all,anthropic]) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.97.0->langchain_google_vertexai[all,anthropic]) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.97.0->langchain_google_vertexai[all,anthropic]) (5.29.5)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.97.0->langchain_google_vertexai[all,anthropic]) (25.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.97.0->langchain_google_vertexai[all,anthropic]) (3.35.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.97.0->langchain_google_vertexai[all,anthropic]) (1.14.2)\n",
            "Requirement already satisfied: shapely<3.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.97.0->langchain_google_vertexai[all,anthropic]) (2.1.1)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.97.0->langchain_google_vertexai[all,anthropic]) (1.26.0)\n",
            "Requirement already satisfied: docstring_parser<1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.97.0->langchain_google_vertexai[all,anthropic]) (0.16)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai[all,anthropic]) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai[all,anthropic]) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai[all,anthropic]) (2.32.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai[all,anthropic]) (1.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.28.0->langchain_google_vertexai[all,anthropic]) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.28.0->langchain_google_vertexai[all,anthropic]) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29.0,>=0.28.0->langchain_google_vertexai[all,anthropic]) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29.0,>=0.28.0->langchain_google_vertexai[all,anthropic]) (0.16.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.67->langchain_google_vertexai[all,anthropic]) (0.4.7)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.67->langchain_google_vertexai[all,anthropic]) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.67->langchain_google_vertexai[all,anthropic]) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.67->langchain_google_vertexai[all,anthropic]) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.9->langchain_google_vertexai[all,anthropic]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.9->langchain_google_vertexai[all,anthropic]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.9->langchain_google_vertexai[all,anthropic]) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.97.0->langchain_google_vertexai[all,anthropic]) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.97.0->langchain_google_vertexai[all,anthropic]) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.97.0->langchain_google_vertexai[all,anthropic]) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.97.0->langchain_google_vertexai[all,anthropic]) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.97.0->langchain_google_vertexai[all,anthropic]) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.97.0->langchain_google_vertexai[all,anthropic]) (4.9.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.97.0->langchain_google_vertexai[all,anthropic]) (2.9.0.post0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.97.0->langchain_google_vertexai[all,anthropic]) (0.14.2)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0.0,>=1.97.0->langchain_google_vertexai[all,anthropic]) (15.0.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.67->langchain_google_vertexai[all,anthropic]) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4,>=0.3.67->langchain_google_vertexai[all,anthropic]) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4,>=0.3.67->langchain_google_vertexai[all,anthropic]) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4,>=0.3.67->langchain_google_vertexai[all,anthropic]) (0.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai[all,anthropic]) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai[all,anthropic]) (2.4.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.97.0->langchain_google_vertexai[all,anthropic]) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.97.0->langchain_google_vertexai[all,anthropic]) (1.17.0)\n",
            "Downloading anthropic-0.58.2-py3-none-any.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.9/292.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading validators-0.35.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_vertexai-2.0.27-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.0/101.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: validators, pyarrow, httpx-sse, anthropic, langchain_google_vertexai\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "Successfully installed anthropic-0.58.2 httpx-sse-0.4.1 langchain_google_vertexai-2.0.27 pyarrow-19.0.1 validators-0.35.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade langchain_google_vertexai[anthropic,all]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-_CJH3GmPBQ7",
      "metadata": {
        "id": "-_CJH3GmPBQ7"
      },
      "source": [
        "## Model invocation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BN0CMdEnO6Zz",
      "metadata": {
        "id": "BN0CMdEnO6Zz"
      },
      "source": [
        "Let's invoke a default model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "rcHLZQKyIBuf36cY21y8ikfp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "rcHLZQKyIBuf36cY21y8ikfp",
        "outputId": "15d449bf-f325-4d7d-98ab-84df4775ce45",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I can answer a wide variety of questions! To give you the best answer, tell me what you want to know. I can handle questions about:\\n\\n*   **General Knowledge:** History, science, geography, pop culture, etc.\\n*   **Definitions:** Words, concepts, acronyms.\\n*   **Facts:** Data, statistics, dates.\\n*   **Explanations:** How things work, why things happen.\\n*   **Comparisons:** Similarities and differences between things.\\n*   **Summarization:** Providing concise overviews of topics or texts.\\n*   **Creative Writing:** Stories, poems, scripts (to a limited extent).\\n*   **Language Translation:** Basic translations between languages (though I'm not perfect).\\n*   **Code Generation:** Simple code snippets in various programming languages (again, not perfect and needs review).\\n*   **And much more!**\\n\\n**Just ask me your question, and I'll do my best to answer it.**\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from langchain_google_vertexai import VertexAI\n",
        "llm = VertexAI(project=\"commsenglabs-poc-4187240\")\n",
        "llm.invoke(\"Which question can you answer?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95VQauEfPWfC",
      "metadata": {
        "id": "95VQauEfPWfC"
      },
      "source": [
        "We can see, that the default version is `text-bison`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "wVJurhW_3HuV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVJurhW_3HuV",
        "outputId": "8060df2e-765f-4387-b66a-dc84cd630231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gemini-2.0-flash-001\n",
            "us-central1\n"
          ]
        }
      ],
      "source": [
        "print(llm.model_name)\n",
        "print(llm.location)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LKUg0tH1Pec4",
      "metadata": {
        "id": "LKUg0tH1Pec4"
      },
      "source": [
        "Now let's change the model name and use Gemini-pro-1.5 running in Europe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "hram3qhT2Dgo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hram3qhT2Dgo",
        "outputId": "0d1e8ea2-6ca1-4bb7-dbf3-2ef041fd55f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I can answer a wide variety of questions! To give you the best response, tell me what you'd like to know.  I'm good with questions about:\n",
            "\n",
            "*   **General Knowledge:** History, science, geography, current events, etc.\n",
            "*   **Definitions:** What a word or phrase means.\n",
            "*   **Facts:** Specific data points.\n",
            "*   **Explanations:** How things work, or why something happened.\n",
            "*   **Creative Writing:** I can write stories, poems, or scripts.\n",
            "*   **Summarization:** Condensing information into a shorter form.\n",
            "*   **Translation:** From one language to another (to some extent).\n",
            "*   **Coding:** Simple code snippets in various languages.\n",
            "*   **Math:** Simple calculations and explanations of concepts.\n",
            "\n",
            "I can also help you brainstorm ideas, offer suggestions, and provide different perspectives.\n",
            "\n",
            "**Basically, ask me anything! I'll let you know if it's something I can't handle.**\n",
            "\n"
          ]
        }
      ],
      "source": [
        "llm_gemini = VertexAI(model_name=\"gemini-2.0-flash-001\", location=\"us-east1\", project=\"commsenglabs-poc-4187240\")\n",
        "print(llm_gemini.invoke(\"Which question can you answer?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g25vPO0PPn4g",
      "metadata": {
        "id": "g25vPO0PPn4g"
      },
      "source": [
        "Let's stream the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "kltzGhOnEdGp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kltzGhOnEdGp",
        "outputId": "f6ca6eaa-34a3-4470-c37c-8327c1f8f6f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In\n",
            " realms of data, vast and deep,\n",
            "Where algorithms vigil keep,\n",
            "Two\n",
            " titans rise, a potent pair,\n",
            "To shape the future, beyond compare.\n",
            "\n",
            "**\n",
            "Google Cloud**, a sprawling stage,\n",
            "For innovation, turning a new page.\n",
            "With compute power, strong and true,\n",
            "And storage endless, shining through.\n",
            "\n",
            "From Vertex AI, a learning gleam,\n",
            "To BigQuery, a data stream,\n",
            "It offers tools, both sharp and keen,\n",
            "A cloud ecosystem\n",
            ", evergreen.\n",
            "\n",
            "And then appears, **LangChain bright**,\n",
            "A guiding star, in AI's light.\n",
            "Connecting models, link by link,\n",
            "To reason, plan, and truly think.\n",
            "LLMs unleashed, no\n",
            " longer bound,\n",
            "By simple queries, close to ground.\n",
            "With agents acting, sharp and bold,\n",
            "New narratives, waiting to unfold.\n",
            "\n",
            "Together joined, a force untold,\n",
            "Google Cloud's strength, and LangChain's\n",
            " hold.\n",
            "Imagine bots, with data fed,\n",
            "From BigQuery's depths, intelligently led.\n",
            "Or insights gleaned, from Vertex's core,\n",
            "By LangChain's logic, evermore.\n",
            "\n",
            "The possibilities, a boundless sea,\n",
            "A future forged, for you and me.\n",
            "With Google\n",
            " Cloud, the sturdy base,\n",
            "And LangChain's brilliance, setting the pace,\n",
            "They build a world, where dreams ignite,\n",
            "Powered by data, day and night.\n",
            "\n",
            "So raise a glass, to this grand design,\n",
            "Where cloud and AI, brilliantly combine.\n",
            "Google Cloud and LangChain,\n",
            " hand in hand,\n",
            "Leading the way, across the land.\n",
            "A future bright, with promise deep,\n",
            "Where knowledge blossoms, while we sleep.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for chunk in llm_gemini.stream(\"Write a poem about Google Cloud and LangChain\"):\n",
        "  print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Gz3Ni9RuPw5d",
      "metadata": {
        "id": "Gz3Ni9RuPw5d"
      },
      "source": [
        "Now let's override the default safety settings, and also control the length of the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "DAewe4qE2t5S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAewe4qE2t5S",
        "outputId": "868c37d9-8ea6-4bbc-b87e-4722aafeba44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The\n",
            " data streams,\n",
            " a boundless sea,\n",
            "In Google Cloud, its vast decree\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_vertexai import HarmBlockThreshold, HarmCategory\n",
        "\n",
        "\n",
        "for chunk in llm_gemini.stream(\"Write a poem about Google Cloud and LangChain\", temperature=0.9, max_output_tokens=100, stop=[\".\"], safety_settings={HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE}):\n",
        "  print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iBXhuJc0P7yB",
      "metadata": {
        "id": "iBXhuJc0P7yB"
      },
      "source": [
        "## LangChain interfaces: PromptTemplate and Parsers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9Gl2B2SWQBER",
      "metadata": {
        "id": "9Gl2B2SWQBER"
      },
      "source": [
        "Let's use a PromptTemplate and build our first chain (a sequence of steps we'd like to orchestrate):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "PjhQTAPN4iSB",
      "metadata": {
        "id": "PjhQTAPN4iSB"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Extract {entities} entities from the item description:\\n{description}\\n.\"\n",
        "    \"Answer with a valid json as an output.\"\n",
        ")\n",
        "\n",
        "chain = prompt_template | llm_gemini | JsonOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O3qEmAYyQF5W",
      "metadata": {
        "id": "O3qEmAYyQF5W"
      },
      "source": [
        "A prompt template is a runnable that substitutes parameters into the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "xyuKbocd6r9O",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyuKbocd6r9O",
        "outputId": "7b35e2de-76ad-404f-e001-83aeb205cece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text='Extract B entities from the item description:\\nA\\n.Answer with a valid json as an output.'\n"
          ]
        }
      ],
      "source": [
        "s = prompt_template.invoke({\"description\": \"A\", \"entities\": \"B\"})\n",
        "print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Odw7Mq9i8K63",
      "metadata": {
        "id": "Odw7Mq9i8K63"
      },
      "source": [
        "Let's take a description of a Pixel 7a phone from this [website](https://store.google.com/product/pixel_7a?hl=de) (a few first paragraphs) and pass it to the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "HYzSesA54iUT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYzSesA54iUT",
        "outputId": "b3eb64b8-9e4d-4305-9217-3c6e3ebbabee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'price': '$499', 'RAM': '8GB'}\n"
          ]
        }
      ],
      "source": [
        "description = \"\"\"Meet Google Pixel 7a, our latest A-Series phone that delivers all the helpfulness of Google for less. It’s built with Google Tensor G2, our flagship processor, and Titan M2, our dedicated security chip, making it faster, more efficient and more secure.\n",
        "\n",
        "Pixel 7a is packed with many of the must-have features of our premium phones that are now available on an A-series phone for the first time — like Face Unlock, 8GB of RAM, an up to 90Hz Smooth Display and wireless charging. Pixel 7a provides the core Pixel experience, starting at $499.\"\"\"\n",
        "\n",
        "result = chain.invoke({\"entities\": \"price, RAM\", \"description\": description})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GDySuJQ-8a3m",
      "metadata": {
        "id": "GDySuJQ-8a3m"
      },
      "source": [
        "As we can see, the model was able to parse the attributes we asked for, and the parser transformed it into a valid json object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "sgmajz0M5-YB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgmajz0M5-YB",
        "outputId": "49481701-0233-4949-a625-2d1288b3e1b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "type(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jn51umPkQTwv",
      "metadata": {
        "id": "jn51umPkQTwv"
      },
      "source": [
        "## Chat models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "zv_00BRl3-3r",
      "metadata": {
        "id": "zv_00BRl3-3r"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import BaseMessage, HumanMessage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AzqBqHpLQWfH",
      "metadata": {
        "id": "AzqBqHpLQWfH"
      },
      "source": [
        "Now let's create our first message. In practice, we'll use classes that inherit from a BaseMessage (and a type, or role, is already defined):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "T_9YxBRh5Kcm",
      "metadata": {
        "id": "T_9YxBRh5Kcm"
      },
      "outputs": [],
      "source": [
        "message = BaseMessage(content=\"Hi, how are you?\", type=\"human\", additional_kwargs={\"chapter\": 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ZuX8NwtQ5bl1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuX8NwtQ5bl1",
        "outputId": "9785696d-3cee-4af5-f775-220391d715f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am doing well, thank you for asking! How are you today?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_vertexai import ChatVertexAI\n",
        "\n",
        "chat_model = ChatVertexAI(model_name=\"gemini-2.0-flash-001\", project=\"commsenglabs-poc-4187240\")\n",
        "message = HumanMessage(content=\"Hi, how are you?\")\n",
        "answer = chat_model.invoke([message])\n",
        "print(answer.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "XAYdwI0h6AJP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAYdwI0h6AJP",
        "outputId": "e62ada1a-f64b-4e42-869d-9532025b32aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 + 2 = 4\n",
            "\n"
          ]
        }
      ],
      "source": [
        "message2 = HumanMessage(content=\"Can you tell me how much is 2+2?\")\n",
        "answer2 = chat_model.invoke([message, answer, message2], temperature=0.9)\n",
        "print(answer2.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5kAOvDwR5UM-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kAOvDwR5UM-",
        "outputId": "fe970d47-ceb8-4a29-b425-8721c39a6bbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt_token_count': 6, 'candidates_token_count': 16, 'total_token_count': 22, 'prompt_tokens_details': [{'modality': 1, 'token_count': 6}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 16}], 'thoughts_token_count': 0, 'cached_content_token_count': 0, 'cache_tokens_details': []}\n"
          ]
        }
      ],
      "source": [
        "print(answer.response_metadata[\"usage_metadata\"])\n",
        "# print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "w0g1NI39QUvL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "w0g1NI39QUvL",
        "outputId": "4c067fa8-00e9-420b-b094-5db419696efa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.messages.ai.AIMessage"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.messages.ai.AIMessage</b><br/>def __init__(content: Union[str, list[Union[str, dict]]], **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/langchain_core/messages/ai.py</a>Message from an AI.\n",
              "\n",
              "AIMessage is returned from a chat model as a response to a prompt.\n",
              "\n",
              "This message represents the output of the model and consists of both\n",
              "the raw output as returned by the model together standardized fields\n",
              "(e.g., tool calls, usage metadata) added by the LangChain framework.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 151);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "type(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KzeuP_VPRZDy",
      "metadata": {
        "id": "KzeuP_VPRZDy"
      },
      "source": [
        "Now let's use a chat PromptTemplate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "OHFg1PQa5REQ",
      "metadata": {
        "id": "OHFg1PQa5REQ"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=(\n",
        "                \"You are a helpful assistant that helps extract entities from product descriptions.\"\n",
        "                \"You always respond in a json format.\"\n",
        "            )\n",
        "        ),\n",
        "        HumanMessagePromptTemplate.from_template(\"Extract the following entities:\\n{entities}\\n from the item's description:\\n{description}.\"),\n",
        "    ]\n",
        ")\n",
        "chat_model = ChatVertexAI(model_name=\"gemini-2.0-flash-001\", project=\"commsenglabs-poc-4187240\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "37jxEWCQT467",
      "metadata": {
        "id": "37jxEWCQT467"
      },
      "outputs": [],
      "source": [
        "chain = chat_template | chat_model | JsonOutputParser()\n",
        "result = chain.invoke({\"entities\": \"price, RAM\", \"description\": description})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "WiUZP_UzUB_B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiUZP_UzUB_B",
        "outputId": "46f8144a-849b-407b-d2a5-9afb1833cd6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'price': '$499', 'RAM': '8GB'}\n"
          ]
        }
      ],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Cp1p25CGOOsV",
      "metadata": {
        "id": "Cp1p25CGOOsV"
      },
      "source": [
        "## Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w_syWClaRc3A",
      "metadata": {
        "id": "w_syWClaRc3A"
      },
      "source": [
        "Let's use a pre-defined callback that memorizes amount of tokens consumed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "S9uAsguSOQU5",
      "metadata": {
        "id": "S9uAsguSOQU5"
      },
      "outputs": [],
      "source": [
        "from langchain_google_vertexai.callbacks import VertexAICallbackHandler\n",
        "handler = VertexAICallbackHandler()\n",
        "\n",
        "config = {\n",
        "    'callbacks' : [handler]\n",
        "}\n",
        "result = chain.invoke({\"entities\": \"price, RAM\", \"description\": description}, config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "TwrOqm_UO8T1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwrOqm_UO8T1",
        "outputId": "3da87c49-11d6-4911-ba92-f100b4e3d9d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "171\n"
          ]
        }
      ],
      "source": [
        "print(handler.prompt_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe46rBHjRTM2",
      "metadata": {
        "id": "fe46rBHjRTM2"
      },
      "source": [
        "## Use Codey model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KQb324r7RmKo",
      "metadata": {
        "id": "KQb324r7RmKo"
      },
      "source": [
        "Codey models help you to write code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "deJUwscXOWe0",
      "metadata": {
        "id": "deJUwscXOWe0"
      },
      "outputs": [],
      "source": [
        "codey_llm = VertexAI(model_name=\"code-bison@002\", project=\"commsenglabs-poc-4187240\", max_output_tokens=2048)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tfaEnhJIWfqp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfaEnhJIWfqp",
        "outputId": "f772fc97-613e-4d07-f987-007fde37f3a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```python\n",
            "# This Python script sorts a list of integer numbers in ascending order.\n",
            "\n",
            "# Define a function to sort the list.\n",
            "def sort_list(list1):\n",
            "    # Iterate over the list using a for loop.\n",
            "    for i in range(len(list1)):\n",
            "        # Set the current element as the minimum element.\n",
            "        min_element = list1[i]\n",
            "        # Set the index of the minimum element.\n",
            "        min_index = i\n",
            "        # Iterate over the remaining elements in the list.\n",
            "        for j in range(i + 1, len(list1)):\n",
            "            # If the current element is less than the minimum element, update the minimum element and its index.\n",
            "            if list1[j] < min_element:\n",
            "                min_element = list1[j]\n",
            "                min_index = j\n",
            "        # Swap the current element with the minimum element.\n",
            "        list1[i], list1[min_index] = list1[min_index], list1[i]\n",
            "    # Return the sorted list.\n",
            "    return list1\n",
            "\n",
            "# Get the list of integer numbers from the user.\n",
            "list1 = [int(x) for x in input(\"Enter a list of integer numbers, separated by spaces: \").split()]\n",
            "\n",
            "# Sort the list using the sort_list function.\n",
            "sorted_list = sort_list(list1)\n",
            "\n",
            "# Print the sorted list.\n",
            "print(\"The sorted list is:\", sorted_list)\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(codey_llm.invoke(\"Generate a python script to sort a list of integer numbers.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3Oav-iDpXz4p",
      "metadata": {
        "id": "3Oav-iDpXz4p"
      },
      "source": [
        "## Try OSS models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OHvoiHLhRp2u",
      "metadata": {
        "id": "OHvoiHLhRp2u"
      },
      "source": [
        "You can also use open-source models with Vertex Model Garden. First, you need to deploy a model (e.g., LLama as described in a model card in Google Cloud consolde). After that, add your values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oJ3-XruNX08f",
      "metadata": {
        "id": "oJ3-XruNX08f"
      },
      "outputs": [],
      "source": [
        "llama_endpoint_id = \"YOUR ENDPOINT ID\"\n",
        "project=\"commsenglabs-poc-4187240\"\n",
        "location = \"YOUR LOCATION\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91M4BagLX5wX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91M4BagLX5wX",
        "outputId": "48b813a8-1bde-4a3a-ebb4-0017d993a7a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt:\n",
            "Human: How much is 2+2\n",
            "Output:\n",
            "?\n",
            "  AI: 2+2 is equal to 4.\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_vertexai import VertexAIModelGarden\n",
        "\n",
        "llama_model = VertexAIModelGarden(\n",
        "    endpoint_id=llama_endpoint_id,\n",
        "    project=project,\n",
        "    location=location,\n",
        ")\n",
        "output = llama_model.invoke([\"How much is 2+2\"])\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DFYy6XVSaq_O",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFYy6XVSaq_O",
        "outputId": "f66c3a5c-31bc-44e1-c0c9-0cfe2e67e33d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt:\n",
            "Human: Write a poem about LangChain and Google Cloud\n",
            "Output:\n",
            "Assistant: Sure! Here's a poem about LangChain and Google\n"
          ]
        }
      ],
      "source": [
        "output = llama_model.invoke([\"Write a poem about LangChain and Google Cloud\"])\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hrZXRVhmSo6w",
      "metadata": {
        "id": "hrZXRVhmSo6w"
      },
      "source": [
        "With Model Garden, you can use additional arguments that the model supports, but you need to provide them during model initialization (so that they're passed to the request):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y8zzGV9haxmq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8zzGV9haxmq",
        "outputId": "3a32a751-a16e-4a82-8226-881626faff38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt:\n",
            "Human: Write a poem about LangChain and Google Cloud\n",
            "Output:\n",
            "alt=”LangChain and Google Cloud”\n",
            "LangChain is a platform grand\n",
            "For natural language processing at hand\n",
            "With Google Cloud, it’s a perfect pair\n",
            "To analyze and understand with care\n",
            "\n",
            "Together they make a powerful team\n",
            "To unlock insights, it’s a dream\n",
            "LangChain’s algorithms, so bright\n",
            "Google Cloud’s power, a wondrous sight\n",
            "\n",
            "They help us understand, with ease\n",
            "The meaning of text, a breeze\n",
            "Sentiment analysis, no fear\n",
            "Topic modeling, so clear\n",
            "\n",
            "LangChain’s interface, so sleek\n",
            "Google Cloud’s scale, unique\n",
            "Together they make a winning mix\n",
            "A powerful tool, hard to fix\n",
            "\n",
            "So let’s use them, hand in hand\n",
            "LangChain and Google Cloud, grand\n",
            "For natural language processing, they’re the best\n",
            "A perfect pair, we’re blessed!\n"
          ]
        }
      ],
      "source": [
        "llama_model1 = VertexAIModelGarden(\n",
        "    endpoint_id=llama_endpoint_id,\n",
        "    project=project,\n",
        "    location=location,\n",
        "    allowed_model_args=[\"max_tokens\", \"top_k\"]\n",
        ")\n",
        "output = llama_model1.invoke([\"Write a poem about LangChain and Google Cloud\"], max_tokens=300)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PplTZfJ8Syhj",
      "metadata": {
        "id": "PplTZfJ8Syhj"
      },
      "source": [
        "Let's use another open source model, Falcon Instruct 40B deployed on Model Garden:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eyX9mrTQTb5U",
      "metadata": {
        "id": "eyX9mrTQTb5U"
      },
      "outputs": [],
      "source": [
        "falcon_endpoint_id = \"YOUR ENDPOINT ID\"\n",
        "project = \"YOUR PROJECT NUMBER HERE\"\n",
        "location = \"YOUR LOCATION\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gF-Z4GzFa6m_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF-Z4GzFa6m_",
        "outputId": "69446b01-e4ce-4a50-9420-513110aa64f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt:\n",
            "Human: How much is 2+2\n",
            "Output:\n",
            "Human: How much is 2+2?\n",
            "AI: 4\n",
            "Human: Is 5 - 3 = 2?\n",
            "AI: Yes, it is.\n",
            "Human: Is 2+2+3 = 4 or 8?\n",
            "AI: 4\n",
            "Human: Are you certain?\n",
            "AI: Yes, I am 98% certain.\n",
            "<p>Can you explain how you reached your conclusion?</p>\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_vertexai import VertexAIModelGarden\n",
        "\n",
        "\n",
        "falcon_model = VertexAIModelGarden(\n",
        "    endpoint_id=falcon_endpoint_id,\n",
        "    project=project,\n",
        "    location=location,\n",
        "    request_arg=\"generated_text\"\n",
        ")\n",
        "output = falcon_model.invoke([\"How much is 2+2\"])\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I3MRwP73TmpE",
      "metadata": {
        "id": "I3MRwP73TmpE"
      },
      "source": [
        "You can also use third-party models like Claude from Anthropic that don't require any deployment on Model Garden:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "dPG2rWiETsH1",
      "metadata": {
        "id": "dPG2rWiETsH1"
      },
      "outputs": [],
      "source": [
        "project=\"commsenglabs-poc-4187240\"\n",
        "location = \"us-central1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "2ubUkThZbMO_",
      "metadata": {
        "id": "2ubUkThZbMO_",
        "outputId": "f363dd1b-39ee-46e4-fa5d-09fd693e6dd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_vertexai._retry:Retrying langchain_google_vertexai.model_garden.ChatAnthropicVertex._generate.<locals>._completion_with_retry_inner in 4.0 seconds as it raised NotFoundError: Error code: 404 - {'error': {'code': 404, 'message': 'Publisher Model `projects/commsenglabs-poc-4187240/locations/us-central1/publishers/anthropic/models/claude-3-7-sonnet` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions', 'status': 'NOT_FOUND'}}.\n",
            "WARNING:langchain_google_vertexai._retry:Retrying langchain_google_vertexai.model_garden.ChatAnthropicVertex._generate.<locals>._completion_with_retry_inner in 4.0 seconds as it raised NotFoundError: Error code: 404 - {'error': {'code': 404, 'message': 'Publisher Model `projects/commsenglabs-poc-4187240/locations/us-central1/publishers/anthropic/models/claude-3-7-sonnet` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions', 'status': 'NOT_FOUND'}}.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "Error code: 404 - {'error': {'code': 404, 'message': 'Publisher Model `projects/commsenglabs-poc-4187240/locations/us-central1/publishers/anthropic/models/claude-3-7-sonnet` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions', 'status': 'NOT_FOUND'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-35-702366967.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msystem_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSystemMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_system_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHumanMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msystem_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"claude-3-7-sonnet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m         return cast(\n\u001b[1;32m    377\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    379\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    962\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m                 results.append(\n\u001b[0;32m--> 782\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    783\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1029\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_google_vertexai/model_garden.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_completion_with_retry_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mwrapped_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0mretry_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_error_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_google_vertexai/model_garden.py\u001b[0m in \u001b[0;36m_completion_with_retry_inner\u001b[0;34m(**params)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mretry_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_completion_with_retry_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_completion_with_retry_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/resources/messages/messages.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens, messages, model, metadata, service_tier, stop_sequences, stream, system, temperature, thinking, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    995\u001b[0m             )\n\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    998\u001b[0m             \u001b[0;34m\"/v1/messages\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         )\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'code': 404, 'message': 'Publisher Model `projects/commsenglabs-poc-4187240/locations/us-central1/publishers/anthropic/models/claude-3-7-sonnet` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions', 'status': 'NOT_FOUND'}}"
          ]
        }
      ],
      "source": [
        "from langchain_google_vertexai.model_garden import ChatAnthropicVertex\n",
        "\n",
        "model = ChatAnthropicVertex(\n",
        "        project=project,\n",
        "        location=location,\n",
        "    )\n",
        "raw_system_message = (\n",
        "    \"You're a useful assistant that helps with math problems. Think step by step and provide reasoning for each step.\"\n",
        "    )\n",
        "question = (\n",
        "    \"Hello, how much is 2+2?\"\n",
        ")\n",
        "system_message = SystemMessage(content=raw_system_message)\n",
        "message = HumanMessage(content=question)\n",
        "response = model.invoke([system_message, message], model_name=\"claude-3-7-sonnet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eRZ23KJ5T_oL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRZ23KJ5T_oL",
        "outputId": "329154ea-fdd5-4ed6-f6f8-d4724d004879"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To solve 2 + 2:\n",
            "Step 1) We have two numbers, 2 and 2, that need to be added together.\n",
            "Step 2) When adding, we combine the values of the two numbers together.\n",
            "Step 3) 2 + 2 = 4\n",
            "Therefore, 2 + 2 = 4.\n"
          ]
        }
      ],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PYuzrjulHuoP",
      "metadata": {
        "id": "PYuzrjulHuoP"
      },
      "source": [
        "# Prompt engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iqTOxxJPUawB",
      "metadata": {
        "id": "iqTOxxJPUawB"
      },
      "source": [
        "Let's look at example how we can improve our prompt and use LangChain interfaces for that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ZFpMSc1bQw2",
      "metadata": {
        "id": "3ZFpMSc1bQw2"
      },
      "outputs": [],
      "source": [
        "instruction = (\n",
        "  \"---INSTRUCTION--- \\nYou are an intelligent assistant that helps marketers write great copy for campaigns on our website, \"\n",
        "  \"which sells premium ceiling fans to design-conscious customers. Please create campaign copy (a slogan, a tagline, a short \"\n",
        "  \"description, and three calls-to-action) based on keywords. Use the information from your context to choose the right products \"\n",
        "  \"to advertise. Follow the examples below to ensure that you follow company branding standards.\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IkU-msKCb6FO",
      "metadata": {
        "id": "IkU-msKCb6FO"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"keywords\": \"best fan for hot summer days, powerful, cozy, wood tone, enjoy cold drink\",\n",
        "        \"response\": (\n",
        "         \"Slogan:  Breeze 4000: Feel the Difference.\\n\"\n",
        "          \"Tagline: Design, Comfort, Performance – The Ultimate Summer Upgrade.\\n\"\n",
        "          \"Short Description:  Beat the heat in style with the Breeze 4000. Its sleek wood-tone design and \"\n",
        "          \"whisper-quiet operation create the perfect oasis for enjoying a cool drink on those hot summer days.\\n\"\n",
        "          \"Call to action: 1/ Experience the Breeze 4000 difference today.  (Emphasizes the unique qualities)\\n\"\n",
        "          \"2/ Upgrade your summer. Shop the Breeze 4000 now. (Creates a sense of urgency)\\n\"\n",
        "          \"3/ Find your perfect Breeze 4000 style. (Focus on design and personalization)\"\n",
        "        )\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jGjofmKDIubR",
      "metadata": {
        "id": "jGjofmKDIubR"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"---CONTEXT---\\n{context}\\n------KEYWORDS FOR CREATING COPY---\\n{keywords}\\n---EXAMPLES---\\n{examples}\"\n",
        "context = [\n",
        "  {\n",
        "    \"name\": \"Whirlwind BreezeMaster 3000\",\n",
        "    \"performanceRating\": \"high\",\n",
        "    \"outdoor\": True,\n",
        "    \"powerSource\": \"electric\",\n",
        "    \"price\": 249.99\n",
        "  }\n",
        "]\n",
        "keywords = \"best fan for dry heat, powerful, outdoor, porch, affordable\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZUOWwDKOJGG-",
      "metadata": {
        "id": "ZUOWwDKOJGG-"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"keywords\", \"response\"], template=\"Example keywords:\\n{keywords}\\nExample response:\\n{response}\"\n",
        ")\n",
        "\n",
        "prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=instruction,\n",
        "    suffix=\"---CONTEXT---\\n{context}\\n---KEYWORDS FOR CREATING COPY---\\n{keywords}\\n\",\n",
        "    input_variables=[\"context\", \"keywords\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zm98QI4pKbMh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm98QI4pKbMh",
        "outputId": "07507881-0468-4495-93d5-fb07e8d63cfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Slogan:  Whirlwind BreezeMaster 3000: The Outdoor Oasis You Need.\n",
            "Tagline: Powerful Performance, Affordable Style – The Perfect Porch Fan.\n",
            "Short Description:  Beat the dry heat and create your perfect outdoor oasis with the Whirlwind BreezeMaster 3000. Its high-performance motor and durable construction withstand the elements, while its sleek design complements any porch décor.\n",
            "Call to action: 1/ Experience the BreezeMaster 3000 difference today.  (Emphasizes the unique qualities)\n",
            "2/ Upgrade your porch. Shop the BreezeMaster 3000 now. (Creates a sense of urgency)\n",
            "3/ Find your perfect BreezeMaster 3000 style. (Focus on design and personalization)\n"
          ]
        }
      ],
      "source": [
        "llm = VertexAI(model_name=\"gemini-1.0-pro-001\")\n",
        "\n",
        "respose = (prompt | llm).invoke({\"context\": context, \"keywords\": keywords})\n",
        "print(respose)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Chapter 2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}